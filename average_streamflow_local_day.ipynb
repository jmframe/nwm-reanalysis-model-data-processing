{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl \n",
    "import sys \n",
    "from pathlib import Path\n",
    "from os.path import basename\n",
    "from timezonefinder import TimezoneFinder\n",
    "import geopy.geocoders, pytz, certifi, ssl, datetime, ephem, math\n",
    "from datetime import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_dir = '/home/NearingLab/projects/jmframe/lstm_camels/'\n",
    "data_dir = proj_dir + 'data/'\n",
    "nldas_dir = data_dir + 'basin_dataset_public_v1p2/basin_mean_forcing/nldas_extended/'\n",
    "chrt_daily_dir = '/home/NearingLab/data/nwm/v2/temp/CHRT_daily_means/'\n",
    "chrt_year_dir = '/home/NearingLab/data/nwm/v2/temp/CHRT_OUT/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False: # Open the attributes from the coding workshop. These are already set up to do regression\n",
    "    openthis = '/home/NearingLab/data/camels_all_coding_workshop.csv'\n",
    "    attributes = pd.read_csv(openthis, sep=',', index_col='gauge_id')\n",
    "    # These are bad for the regression analysis.\n",
    "    attributes = attributes.drop(drop_these, axis=1)\n",
    "else: # Open a slightly more extrnsive data set.\n",
    "    openthis = '/home/NearingLab/data/camels_attributes_v2.0/camels_all.txt'\n",
    "    attributes = pd.read_csv(openthis, sep=';', index_col='gauge_id')\n",
    "    \n",
    "# Add the basin ID as a 8 element string with a leading zero if neccessary\n",
    "basin_id_str = []\n",
    "for a in attributes.index.values:\n",
    "    basin_id_str.append(str(a).zfill(8))\n",
    "attributes['basin_id_str'] = basin_id_str\n",
    "\n",
    "# Get the hydrologic units for each basin.\n",
    "with open(data_dir + 'usgs_site_info.csv', 'r') as f:\n",
    "    usgs_sites = pd.read_csv(f, skiprows=24, index_col='site_no')\n",
    "usgs_idx_int = []\n",
    "for idx in usgs_sites.index.values:\n",
    "    usgs_idx_int.append(int(idx))\n",
    "usgs_sites.reindex(usgs_idx_int)\n",
    "usgs_sites = usgs_sites.reindex(usgs_idx_int)\n",
    "basin_hydro_unit = []\n",
    "for b in attributes.basin_id_str.values:\n",
    "    huc_cd = usgs_sites.loc[int(b),'huc_cd']\n",
    "    hu = '{:08d}'.format(huc_cd)\n",
    "    basin_hydro_unit.append(hu[0:2])\n",
    "attributes['basin_hydro_unit'] = basin_hydro_unit\n",
    "# Add time zone\n",
    "tf = TimezoneFinder()\n",
    "basin_time_zone = []\n",
    "for b in attributes.index.values:\n",
    "    basin_time_zone.append(tf.timezone_at(lng=attributes.loc[b,'gauge_lon'], lat=attributes.loc[b,'gauge_lat']))\n",
    "attributes['time_zone'] = basin_time_zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/51738137/...\n",
    "# python-finding-local-mean-time-adjusted-for-the-distance-in-longitude-from-the\n",
    "def position(city, state, country):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    :param city: String of city Ex. Chattannoga\n",
    "    :param state: String of state Ex. TN\n",
    "    :param country: String USA\n",
    "    :return: latitude and longitude\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    ctx = ssl.create_default_context(cafile=certifi.where())\n",
    "    geopy.geocoders.options.default_ssl_context = ctx\n",
    "    geo_locator = geopy.geocoders.Nominatim(user_agent=\"my-application\", scheme='http')\n",
    "    location = geo_locator.geocode(city + ' ' + state + ' ' + country)\n",
    "\n",
    "    return location.longitude, location.latitude\n",
    "\n",
    "\n",
    "def timezone(longitude, latitude):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    :param longitude:\n",
    "    :param latitude:\n",
    "    :return: timezone\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    tf = TimezoneFinder()\n",
    "    zone = (tf.certain_timezone_at(lng=longitude, lat=latitude))\n",
    "\n",
    "    return zone\n",
    "\n",
    "# def localmeantime(longitude):\n",
    "#     zones = [-0, -15, -30, -45, -60, -75, -90, -105, -120, -135, -150, -165, -180]\n",
    "#     x = min(zones, key=lambda x: abs(x - longitude))\n",
    "#     print(x)\n",
    "\n",
    "\n",
    "\n",
    "def localtoutc(time, timezo):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    :param time: Ex. \"2001-2-3 10:11:12\" String\n",
    "    :param timezo: Ex. America/New_York String\n",
    "    :return:\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    local = pytz.timezone(timezo)\n",
    "    naive = datetime.datetime.strptime(time, \"%Y-%m-%d %H:%M:%S\")\n",
    "    local_dt = local.localize(naive, is_dst=None)\n",
    "    utc_dt = local_dt.astimezone(pytz.utc)\n",
    "    return utc_dt\n",
    "\n",
    "\n",
    "def localmeantime(utc, longitude):\n",
    "    \"\"\"\n",
    "    :param utc: string Ex. '2008-12-2'\n",
    "    :param longitude: longitude\n",
    "    :return: Local Mean Time Timestamp\n",
    "    \"\"\"\n",
    "    lmt = utc + datetime.timedelta(seconds=round(4*60*longitude))\n",
    "    lmt = lmt.replace(tzinfo=None)\n",
    "    return lmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(chrt_daily_dir+'dynamic_features_v2.p', 'rb') as f:\n",
    "    chrt_v2 = pkl.load(f)\n",
    "chrt_v2_loc = copy.deepcopy(chrt_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_local_time(chrt_year):\n",
    "    with open(chrt_year_dir+'dynamic_features_'+str(y)+'.p', 'rb') as f:\n",
    "        chrt_year = pkl.load(f)\n",
    "        \n",
    "    for ib, b in enumerate(chrt_year.keys()):\n",
    "        df = chrt_year[b]\n",
    "        longitude = attributes.loc[int(b), 'gauge_lon']\n",
    "        if ib == 0:\n",
    "            loc_time = []\n",
    "            for t in df.index.values:\n",
    "                t_pd = pd.Timestamp(t)\n",
    "                t_dt = dt(t_pd.year, t_pd.month, t_pd.day, t_pd.hour)\n",
    "                loc_time.append(localmeantime(t_dt, longitude))\n",
    "        df['loc_time'] = loc_time\n",
    "        df = df.set_index('loc_time')\n",
    "        df = df.groupby(pd.Grouper(freq='1D')).mean()\n",
    "        for variable in ['streamflow','q_lateral','velocity','qSfcLatRunoff','qBucket','qBtmVertRunoff']:\n",
    "            chrt_v2_loc[b].loc[df.index.values[1:], variable] = df[variable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "for y in range(1993, 2019):\n",
    "    add_local_time(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.648749873740599\n",
      "5.45999987795949\n"
     ]
    }
   ],
   "source": [
    "k = list(chrt_v2.keys())[100]\n",
    "i=1050\n",
    "j=0\n",
    "print(chrt_v2[k].iloc[i,j])\n",
    "print(chrt_v2_loc[k].iloc[i,j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(chrt_daily_dir+'dynamic_features_v2_loc.p', 'wb') as f:\n",
    "    pkl.dump(chrt_v2_loc,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
