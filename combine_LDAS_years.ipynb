{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through the nwm land surface files (LDAS)\n",
    "# and combine all the years for each basin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.dates import DateFormatter\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import sys\n",
    "import time\n",
    "import tools\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_v2_dir = '/.../v2/'\n",
    "data_v2_dir_cam = '/.../v2/LDAS/camels_basins/'\n",
    "proj_data_dir = '/.../data/'\n",
    "combine_years = True\n",
    "dis_cumulate = True\n",
    "average_days = Ture\n",
    "start_end_year_list = [1993, 2018]\n",
    "rest_of_years_list = [y for y in range(start_end_year_list[0]+1, start_end_year_list[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDAS years not loaded here\n"
     ]
    }
   ],
   "source": [
    "# Here I am going to load all of the individual years with the collated data, \n",
    "# then put it together.\n",
    "df = {}\n",
    "# Load these as a samples no matter what, we want their start/end dates:\n",
    "file_name_prefix = 'dynamic_features_nwm_ldas_'\n",
    "for y in start_end_year_list:\n",
    "    with open(data_v2_dir_cam+file_name_prefix+str(y)+'.p', 'rb') as pf:\n",
    "        df[y] = pickle.load(pf)\n",
    "\n",
    "# Then the rest of the individual years, if needed\n",
    "if combine_years:\n",
    "    for y in rest_of_years_list:\n",
    "        with open(data_v2_dir_cam+file_name_prefix+str(y)+'.p', 'rb') as pf:\n",
    "            df[y] = pickle.load(pf)\n",
    "else:\n",
    "    print('LDAS years not loaded here')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "basins = list(df[1993].keys())\n",
    "sample_basin = basins[0]\n",
    "firstdt = df[start_end_year_list[0]][sample_basin].index.values[0]\n",
    "lastd = df[start_end_year_list[1]][sample_basin].index.values[-1]\n",
    "date_listn= pd.date_range(start=firstdt, end=lastd, freq='3H')\n",
    "feature_names = list(df[1993][sample_basin].columns.values)\n",
    "n_dates = date_listn.shape[0]\n",
    "n_features = len(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDAS years not combined here\n"
     ]
    }
   ],
   "source": [
    "# Set up the dictionary with a dataframe for each basin,\n",
    "# then append in the year values from our files.\n",
    "if combine_years:\n",
    "    ldas = {b:pd.DataFrame(columns=feature_names) for b in basins}\n",
    "    for b in basins:\n",
    "        for y in df.keys():\n",
    "            ldas[b] = ldas[b].append(df[y][b])\n",
    "        ldas[b] = ldas[b].sort_index()\n",
    "else:\n",
    "    print('LDAS years not combined here')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate date/times not removed here\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicate values, \n",
    "# since time zero shows up at the beginning and end of year, \n",
    "# leading to overlap.\n",
    "if combine_years:\n",
    "    print(ldas[sample_basin].iloc[2914:2928])\n",
    "    for b in basins:\n",
    "        ldas[b] = ldas[b].sort_index()\n",
    "        # dropping duplicate values \n",
    "        ldas[b].drop_duplicates(keep='first',inplace=True) \n",
    "else:\n",
    "    print('Duplicate date/times not removed here')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bad values not filled with NaNs here\n"
     ]
    }
   ],
   "source": [
    "# Replace the -999.. values with non numbers\n",
    "if combine_years:\n",
    "    for b in tqdm(basins):\n",
    "        ldas[b] = ldas[b].apply(lambda x: [y if y > -990 else np.nan for y in x])\n",
    "else:\n",
    "    print('Bad values not filled with NaNs here')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reset dates not set here\n"
     ]
    }
   ],
   "source": [
    "# Mark the dates/times for a reset.\n",
    "if dis_cumulate:\n",
    "    good_reset_dates = {}\n",
    "    previous_reset_date_available = False\n",
    "    for b_count, b in enumerate(basins):\n",
    "        reset_dates = []\n",
    "        for i, d in enumerate(ldas[b].index.values):\n",
    "            if i > 1:\n",
    "                d_1_back = ldas[b].index.values[i-1]\n",
    "                ts_day = pd.to_datetime(d).day\n",
    "                ts_hour = pd.to_datetime(d).hour\n",
    "                ts_month = pd.to_datetime(d).month\n",
    "\n",
    "                if np.isnan(ldas[b].loc[d_1_back, 'ACCET']) or np.isnan(ldas[b].loc[d, 'ACCET']):\n",
    "                    continue\n",
    "                if np.isnan(ldas[b].loc[d_1_back, 'UGDRNOFF']) or np.isnan(ldas[b].loc[d, 'UGDRNOFF']):\n",
    "                    continue\n",
    "\n",
    "                elif ldas[b].loc[d, 'UGDRNOFF'] - ldas[b].loc[d_1_back, 'UGDRNOFF'] == 0:\n",
    "                    break\n",
    "                elif ldas[b].loc[d, 'UGDRNOFF'] - ldas[b].loc[d_1_back, 'UGDRNOFF'] < 0:\n",
    "                    reset_dates.append(d)\n",
    "\n",
    "        if i > ldas[b].index.values.shape[0] - 2:\n",
    "            good_reset_dates[b] = reset_dates\n",
    "            if previous_reset_date_available:\n",
    "                if reset_dates != previous_reset_date:\n",
    "                    print('reset dates differ at basins {}'.format(b))\n",
    "            previous_reset_date = reset_dates\n",
    "            previous_reset_date_available = True\n",
    "\n",
    "        if b_count > 10:\n",
    "            break\n",
    "    print(good_reset_dates)\n",
    "else:\n",
    "    print('Reset dates not set here')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading in LDAS data as ldas2\n"
     ]
    }
   ],
   "source": [
    "# Copy ldas, because we want to save the cumulative values, just in case.\n",
    "if dis_cumulate:\n",
    "    ldas2 = copy.deepcopy(ldas)\n",
    "    # subtract out the time step values from the features with cumulative sums (e.g., ACCET & UGDRNOFF)\n",
    "    for b_count, b in enumerate(basins):\n",
    "        for i, d in enumerate(ldas[b].index.values):\n",
    "            d_1_back = ldas[b].index.values[i-1]\n",
    "            if i > 1:\n",
    "                for dfn in ['ACCET', 'UGDRNOFF']: # dfn = dynamic feature name\n",
    "                    if np.isnan(ldas[b].loc[d_1_back, dfn]):\n",
    "                        ldas2[b].loc[d, dfn] = np.nan\n",
    "                    elif d in reset_dates:\n",
    "                        ldas2[b].loc[d, dfn] = ldas[b].loc[d, dfn]\n",
    "                    elif d not in reset_dates:\n",
    "                        ldas2[b].loc[d, dfn] = ldas[b].loc[d, dfn] - ldas[b].loc[d_1_back, dfn]\n",
    "\n",
    "        # Save file with the 3 hour intervals\n",
    "        with open(data_v2_dir+'ldas_v2_3h.p', 'wb') as pf:\n",
    "            pickle.dump(ldas2, pf, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "else:\n",
    "    # Load file with the 3 hour intervals\n",
    "    print('loading in LDAS data as ldas2')\n",
    "    with open(data_v2_dir+'ldas_v2_3h.p', 'rb') as pf:\n",
    "        ldas2 = pickle.load(pf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data averaged by day\n"
     ]
    }
   ],
   "source": [
    "# Now average the basins by day, instead of 3 hour intervals.\n",
    "if average_days:\n",
    "    def daily_average(df):\n",
    "        return(df.groupby(pd.Grouper(freq='1D')).mean())\n",
    "    ldas_v2_1day_ave = {k: daily_average(v) for (k, v) in ldas2.items()}\n",
    "    \n",
    "    for b in basins:\n",
    "        ldas_v2_1day_ave[b].index.name = 'time'\n",
    "    \n",
    "    with open(data_v2_dir+'nwm_ldas_v2_1d.p', 'wb') as f:\n",
    "        pickle.dump(ldas_v2_1day_ave, f)\n",
    "else:\n",
    "    print('Load data averaged by day')\n",
    "    with open(data_v2_dir+'nwm_ldas_v2_1d.p', 'rb') as pf:\n",
    "        ldas_v2_1day_ave = pickle.load(pf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
